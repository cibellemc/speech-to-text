<h1 align="center"> Dashboard para transcriÃ§Ã£o de Ã¡udio </h1>

<p align="center">
Projeto base: https://github.com/jojojaeger/whisper-streamlit
</p>
<br>

<!-- pasta para o git conseguir acessar a foto -->
<p align="center">
  <img alt="Imagem da tela base do dashboard de transcriÃ§Ã£o de Ã¡udios" src=".github/dashboard-transcricao.png" width="100%">
</p>

## ğŸš€ Tecnologias
- Streamlit: para a criaÃ§Ã£o do web app.
- [Whisper - OpenAI](https://github.com/openai/whisper): modelo de machine learning para reconhecimento e transcriÃ§Ã£o de voz.
- [WhisperX - m-bain](https://github.com/m-bain/whisperX): modelo otimizado, com funÃ§Ã£o de rotulaÃ§Ã£o de falantes.

## ğŸ’» AtualizaÃ§Ãµes
- ParÃ¢metro n_mels: no mÃ©todo whisper.log_mel_spectrogram define o nÃºmero de filtros de Mel (ou bandas de frequÃªncias) que serÃ£o gerados a partir do Ã¡udio. Para o modelo large, o n_mels esperado Ã© 128, enquanto para os modelos de tiny Ã  medium, 80.
- Remover funÃ§Ãµes de transcriÃ§Ã£o para o inglÃªs e representaÃ§Ã£o de pausas: se mostram desnecessÃ¡rias ao contexto.
- AcrÃ©scimo da diarizaÃ§Ã£o do Ã¡udio: uso de [pyannote-audio](https://github.com/pyannote/pyannote-audio) ou whisperX para identificaÃ§Ã£o dos falantes.
- ModificaÃ§Ã£o do layout para mostrar processamento.
- Exibir o Ã¡udio depois do upload: Ãºtil para conferÃªncia.
